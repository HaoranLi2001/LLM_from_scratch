{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QB3SN0TjrcMj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class basicExpert(nn.Module):\n",
        "  def __init__(self, feature_in, feature_out):\n",
        "    super().__init__()\n",
        "\n",
        "    self.expert = nn.Linear(feature_in, feature_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.expert(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "NjtwsesdJ1H8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[3,2],[1,3],[1,2],[2,1]])\n",
        "print(a)\n",
        "b = F.one_hot(a, 4)\n",
        "b = b.permute(2,1,0)\n",
        "\n",
        "idx, top_x = torch.where(b[3])\n",
        "\n",
        "print(f'idx: {idx}, top_x: {top_x}')\n",
        "\n",
        "current = a[top_x, :].reshape(-1, 4)\n",
        "print(current)\n",
        "\n",
        "current_sqz = a.unsqueeze(\n",
        "    0\n",
        ")[:, top_x, :].reshape(-1, 4)\n",
        "print(current_sqz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpsrQoGW9ekT",
        "outputId": "f9b70617-d0a3-4b54-b955-37241903e4e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 2],\n",
            "        [1, 3],\n",
            "        [1, 2],\n",
            "        [2, 1]])\n",
            "idx: tensor([0, 1]), top_x: tensor([0, 1])\n",
            "tensor([[3, 2, 1, 3]])\n",
            "tensor([[3, 2, 1, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Router(nn.Module):\n",
        "  def __init__(self, embedding_size, num_experts, top_k):\n",
        "    super().__init__()\n",
        "    self.gate = nn.Linear(embedding_size, num_experts)\n",
        "    self.num_experts = num_experts\n",
        "    self.topk = top_k\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x.shape = (b*s,embedding), weight.shape = (b*s,num_experts)\n",
        "    weight = self.gate(x)\n",
        "    probs = F.softmax(weight, dim = 1)\n",
        "\n",
        "    # dim=-1: select on the last dim\n",
        "    topk_val, selected_experts = torch.topk(probs, k=self.topk, dim=-1)\n",
        "\n",
        "    # shape = (b*s, top_k)\n",
        "    topk_weights = topk_val / topk_val.sum(dim=1,keepdim=True)\n",
        "    topk_weights = topk_weights.to(x.dtype)\n",
        "\n",
        "    # mask.shape = (b*s, top_k, num_experts)\n",
        "    expert_mask = F.one_hot(\n",
        "        selected_experts,\n",
        "        self.num_experts\n",
        "    )\n",
        "    # mask.shape = (num_experts, top_k, b*s)\n",
        "    expert_mask = expert_mask.permute(2,1,0)\n",
        "\n",
        "    return topk_weights, selected_experts, expert_mask\n",
        "\n",
        "@dataclass\n",
        "class MOEconfig:\n",
        "  num_experts: int = 8\n",
        "  top_k: int = 2\n",
        "  embedding_size: int = 768\n",
        "\n",
        "class sparseMOE(nn.Module):\n",
        "  def __init__(self, num_experts, embedding_size, top_k):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.experts = nn.ModuleList(\n",
        "        [basicExpert(embedding_size, embedding_size)\n",
        "        for _ in range(num_experts)]\n",
        "    )\n",
        "    self.num_experts = num_experts\n",
        "    self.topk = top_k\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch, seq_length, embedding_size = x.size()\n",
        "\n",
        "    x = x.view(-1, embedding_size) # (b*s, embedding_size)\n",
        "\n",
        "    router = Router(embedding_size, self.num_experts, self.topk)\n",
        "    topk_weights, selected_experts, expert_mask = router(x)\n",
        "\n",
        "    hidden_states = torch.zeros(\n",
        "        [batch*seq_length, embedding_size],\n",
        "        dtype = x.dtype\n",
        "    )\n",
        "    for expert_idx in range(self.num_experts):\n",
        "      expert_layer = self.experts[expert_idx]\n",
        "      # idx: 0/1 -> top1/top2 topx: 0,1,2,...,num_experts -> which experts\n",
        "      idx, topx = torch.where(expert_mask[expert_idx])\n",
        "      current_state = x[topx, :].reshape(-1, embedding_size)\n",
        "      current_hidden_states = expert_layer(\n",
        "          current_state\n",
        "      ) * topk_weights[topx, idx].unsqueeze(-1)\n",
        "\n",
        "      hidden_states.index_add_(0, topx, current_hidden_states).to(x.dtype)\n",
        "\n",
        "    hidden_states = hidden_states.reshape(batch, seq_length, embedding_size)\n",
        "\n",
        "    return hidden_states\n",
        "\n",
        "def test_sparseMOE(x, num_experts, embedding_size, top_k):\n",
        "  moe = sparseMOE(num_experts, embedding_size, top_k)\n",
        "  output = moe(x)\n",
        "  return output\n",
        "\n",
        "x = torch.rand([3,2,16])\n",
        "# print(x)\n",
        "output = test_sparseMOE(x, num_experts = 4, embedding_size = 16, top_k = 2)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "zLbYlgbu3_z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37070c31-76fd-497f-9ed8-4dccf198257f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 16])\n"
          ]
        }
      ]
    }
  ]
}